{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open AI Gym - Scratch Pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic Environment methods:**\n",
    "* reset(self): Reset and returns observation.\n",
    "* step(self, action): run one time steps and returns observation, reward, done, info\n",
    "* render(self, mode='human', close=False): render one frame (*mode E {'human','rgb_array','ansi'}*)\n",
    "\n",
    "**Check Spaces**\n",
    "* Action space: env.action_space\n",
    "* Observation space: env.observation_space\n",
    "\n",
    "** Check Environemnts**\n",
    "* Check all available environments: gym.envs.registry.env_specs or gym.envs.registry.all()\n",
    "* CartPole v0 State Representation:  [Cart Position, Cart Velocity, Pole Angle, Pole Velocity at Tip] [Cart Pole Wiki](https://github.com/openai/gym/wiki/CartPole-v0)\n",
    "* Pendulum V0 observation and actions space specs: [Pendulum Wiki](https://github.com/openai/gym/wiki/Pendulum-v0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T15:13:52.855985Z",
     "start_time": "2018-07-09T15:13:52.814653Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T15:16:40.010993Z",
     "start_time": "2018-07-09T15:16:39.989516Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"Pendulum-v0\")\n",
    "env.env.max_speed\n",
    "# env.env.world.gravity = (10,10)\n",
    "# env.env.world.gravity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T16:31:28.730535Z",
     "start_time": "2018-07-09T16:31:28.615593Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"BipedalWalker-v2\")\n",
    "env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T16:19:54.560718Z",
     "start_time": "2018-07-09T16:19:51.021311Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\n",
      "Episode No.: 0\n",
      "-0.784925490061\n",
      "-0.7561697546\n",
      "-0.721445262312\n",
      "-0.68386783288\n",
      "-0.648362895513\n",
      "-0.622600857927\n",
      "-0.618473749601\n",
      "-0.654411989982\n",
      "-0.758991626338\n",
      "-0.976463986609\n",
      "-1.37496034113\n",
      "-2.05786986991\n",
      "-3.17740628403\n",
      "-4.94480288347\n",
      "-7.62121823769\n",
      "-9.28805463564\n",
      "-11.1199397777\n",
      "-13.3698895168\n",
      "-15.9792840728\n",
      "-14.0028440495\n",
      "-11.8515888629\n",
      "-10.1084004551\n",
      "-8.72618411122\n",
      "-7.67249042429\n",
      "-6.93353193005\n",
      "-6.51508789747\n",
      "-6.44094317483\n",
      "-6.74817940128\n",
      "-7.47803957267\n",
      "-8.66227356443\n",
      "-10.3084468193\n",
      "-12.3917259974\n",
      "-14.8600456038\n",
      "-15.0248826761\n",
      "-12.6891959493\n",
      "-10.7834701858\n",
      "-9.25680169193\n",
      "-8.0699623192\n",
      "-7.20118541256\n",
      "-6.64820028326\n",
      "-6.42770457843\n",
      "-6.57210685051\n",
      "-7.12235526093\n",
      "-8.11598444908\n",
      "-9.57213208031\n",
      "-11.4796016542\n",
      "-13.7960251178\n",
      "-16.1294987057\n",
      "-13.600400922\n",
      "-11.5232897556\n",
      "-9.84538791322\n",
      "-8.52165416636\n",
      "-7.52267599848\n",
      "-6.83804492685\n",
      "-6.47682964228\n",
      "-6.46555189682\n",
      "-6.84280147247\n",
      "-7.64925768484\n",
      "-8.91347873214\n",
      "-10.6377169963\n",
      "-12.7916561397\n",
      "-15.3200818638\n",
      "-14.5890529547\n",
      "-12.3313548508\n",
      "-10.4944180324\n",
      "-9.02871714267\n",
      "-7.89775074028\n",
      "-7.08304876681\n",
      "-6.58571590334\n",
      "-6.42548277156\n",
      "-6.63687236088\n",
      "-7.26122602117\n",
      "-8.33399141007\n",
      "-9.86971310698\n",
      "-11.8514726983\n",
      "-14.2325224531\n",
      "-15.6588430286\n",
      "-13.2114141528\n",
      "-11.2068435434\n",
      "-9.59287027335\n",
      "-8.32673677389\n",
      "-7.38216353934\n",
      "-6.75213416794\n",
      "-6.44894891479\n",
      "-6.50170987599\n",
      "-6.95021821506\n",
      "-7.83415680513\n",
      "-9.17844934243\n",
      "-10.9798315856\n",
      "-13.2027390554\n",
      "-15.7894002969\n",
      "-14.1673234971\n",
      "-11.9860196302\n",
      "-10.2163743902\n",
      "-8.81054409065\n",
      "-7.73489792688\n",
      "-6.97430587532\n",
      "-6.53322086367\n",
      "-6.43429596\n",
      "-6.71393370967\n",
      "-7.41348532994\n",
      "-8.56584784598\n",
      "-10.1806371704\n",
      "-12.2352567382\n",
      "-14.6790543713\n",
      "-15.2028654607\n",
      "-12.8356052794\n",
      "-10.9019899104\n",
      "-9.35065739285\n",
      "-8.14133633099\n",
      "-7.25095925855\n",
      "-6.6759031021\n",
      "-6.43162765845\n",
      "-6.5496372754\n",
      "-7.07062148425\n",
      "-8.03281899658\n",
      "-9.4570978097\n",
      "-11.3345404134\n",
      "-13.624645948\n",
      "-16.2677083667\n",
      "-13.7594316159\n",
      "-11.6529142178\n",
      "-9.94911580296\n",
      "-8.60214615726\n",
      "-7.58136786844\n",
      "-6.87502128296\n",
      "-6.49086809487\n",
      "-6.45435506968\n",
      "-6.80350403961\n",
      "-7.57927027577\n",
      "-8.81154017486\n",
      "-10.504715095\n",
      "-12.6306438142\n",
      "-15.1353002179\n",
      "-14.7613358428\n",
      "-12.4726937647\n",
      "-10.6084778427\n",
      "-9.11857429115\n",
      "-7.96537462943\n",
      "-7.12908679017\n",
      "-6.60947054644\n",
      "-6.42505800295\n",
      "-6.60955481717\n",
      "-7.20419001388\n",
      "-8.24529997997\n",
      "-9.74930565842\n",
      "-11.7015720538\n",
      "-14.0570462466\n",
      "-15.8449616002\n",
      "-13.3651061094\n",
      "-11.3317678843\n",
      "-9.6924321077\n",
      "-8.40340375693\n",
      "-7.43714219374\n",
      "-6.78527605211\n",
      "-6.45882333318\n",
      "-6.48587615278\n",
      "-6.90578934823\n",
      "-7.75869653019\n",
      "-9.07102423003\n",
      "-10.8417307706\n",
      "-13.0373136271\n",
      "-15.6009490135\n",
      "-14.3340083293\n",
      "-12.1223993979\n",
      "-10.3260669871\n",
      "-8.89646173975\n",
      "-7.79879099667\n",
      "-7.01658700191\n",
      "-6.55297005692\n",
      "-6.4294409463\n",
      "-6.68168164971\n",
      "-7.3510869026\n",
      "-8.47162749101\n",
      "-10.054924149\n",
      "-12.0806367664\n",
      "-14.4996111359\n",
      "-15.3831509851\n",
      "-12.9840700532\n",
      "-11.0223170171\n",
      "-9.44612729709\n",
      "-8.21421439585\n",
      "-7.30222011975\n",
      "-6.70516734141\n",
      "-6.43726411813\n",
      "-6.52907858251\n",
      "-7.02098602515\n",
      "-7.95185615302\n",
      "-9.34422483151\n",
      "-11.1914432598\n",
      "-13.4549390889\n",
      "-16.0757024097\n",
      "-13.920624442\n",
      "-11.7844445145\n",
      "-10.0545288513\n",
      "-8.68417675619\n",
      "-7.64154237836\n",
      "-6.91351774397\n",
      "-6.506549199\n",
      "-6.44498489598\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# env = gym.make('MountainCarContinuous-v0')\n",
    "# env = gym.make(\"CarRacing-v0\")\n",
    "# env = gym.make(\"LunarLanderContinuous-v2\")\n",
    "# env = gym.make(\"BipedalWalker-v2\")\n",
    "# env.env.world.gravity = (-6,-10)\n",
    "env = gym.make(\"Pendulum-v0\")\n",
    "env.env.max_torque = -5\n",
    "env.env.max_speed = 8\n",
    "\n",
    "num_episodes = 1\n",
    "num_time_steps = 200\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    print(\"\\nEpisode No.: {}\".format(episode))\n",
    "    observation = env.reset()\n",
    "    \n",
    "    for t in range(num_time_steps):\n",
    "#         print(observation)\n",
    "        env.render()\n",
    "        action = env.action_space.sample() # Uniform selection across\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        print(reward)\n",
    "        \n",
    "        if done:\n",
    "#             print(\"Episode finished after {} timesteps\\n\\n\".format(t+1))\n",
    "            break\n",
    "    \n",
    "env.close() # Close windows (and also destroys environment :( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## CEM (Binary Policy - Cart Pole) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T23:44:29.913392Z",
     "start_time": "2018-06-12T23:44:29.575268Z"
    },
    "code_folding": [
     4,
     16,
     51,
     69
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Simple Policy\n",
    "class BinaryActionLinearPolicy(object):\n",
    "    # Policy --> Linear weighted combination of the observations space\n",
    "    # thresholded at zero\n",
    "    def __init__(self, theta):\n",
    "        self.w = theta[:-1]\n",
    "        self.b = theta[-1]\n",
    "        \n",
    "    def act(self, ob):\n",
    "        y = ob.dot(self.w) + self.b\n",
    "        a = int(y < 0)\n",
    "        return a\n",
    "\n",
    "def cem(f, num_steps, th_mean, batch_size, n_iter, elite_frac, initial_std=1.0):\n",
    "    \"\"\"\n",
    "    Generic implementation of the cross-entropy method for maximizing a black-box function\n",
    "    f: a function mapping from vector -> scalar (i.e. from thetas --> total Reward)\n",
    "    num_steps: number of time steps that 'f' will use to evaluate each parameter\n",
    "    th_mean: initial mean of parameters (thetas)\n",
    "    batch_size: number of samples of theta to evaluate per iteration\n",
    "    n_iter: number of iterations\n",
    "    elite_frac: each iteration, select this fraction of the top-performing samples\n",
    "    initial_std: initial standard deviation over parameter (theta) vectors\n",
    "    \"\"\"\n",
    "    \n",
    "    n_elite = int(np.round(batch_size*elite_frac))\n",
    "    th_std = np.ones_like(th_mean) * initial_std # each individual theta std.\n",
    "\n",
    "    for _ in range(n_iter):\n",
    "        \n",
    "        # ths: batch_size x th_mean.size matrix containing in each row a set of\n",
    "        # parameters to be evaluated \n",
    "        ths = np.array([th_mean + dth for dth in  th_std[None,:]*np.random.randn(batch_size, th_mean.size)])\n",
    "        \n",
    "        # Evaluate each sample\n",
    "        ys = np.array([f(th, num_steps) for th in ths])\n",
    "        \n",
    "        # Select top best samples\n",
    "        elite_inds = ys.argsort()[::-1][:n_elite]\n",
    "        elite_ths = ths[elite_inds]\n",
    "#         print(\"Num. Elite: {}\".format(elite_ths.shape[0]))\n",
    "\n",
    "        # Compute new mean and std. by obtaining the avg. of the top (elite) samples\n",
    "        th_mean = elite_ths.mean(axis=0)\n",
    "        th_std = elite_ths.std(axis=0)\n",
    "        yield {'ys' : ys, 'theta_mean' : th_mean, 'y_mean' : ys.mean()}\n",
    "        \n",
    "def do_rollout(agent, env, num_steps, render=False):\n",
    "    \"\"\"\n",
    "    Simulate current agent/policy for 'num_steps' and return the total (undisccounted)\n",
    "    reward and elapsed time steps before episode ended\n",
    "    \"\"\"\n",
    "    \n",
    "    total_rew = 0\n",
    "    ob = env.reset()\n",
    "    for t in range(num_steps):\n",
    "        a = agent.act(ob)\n",
    "        (ob, reward, done, _info) = env.step(a)\n",
    "        total_rew += reward\n",
    "        if render and t%2==0: env.render(mode='human')#rgb_array\n",
    "        if done: break\n",
    "            \n",
    "    return total_rew, t+1\n",
    "\n",
    "def agent_eval(theta, num_steps):\n",
    "    \"\"\"\n",
    "    Given a policy (theta), evaluate it by simulating a single episode runned by\n",
    "    'num_steps' time steps of the simulation,and obtaining the total reward\n",
    "    \"\"\"\n",
    "    agent = BinaryActionLinearPolicy(theta)\n",
    "    rew, T = do_rollout(agent, env, num_steps)\n",
    "    \n",
    "    return rew\n",
    "\n",
    "# def writefile(fname, s):\n",
    "#     with open(path.join(outdir, fname), 'w') as fh: fh.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T23:46:38.157468Z",
     "start_time": "2018-06-12T23:46:36.702839Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "Tried to reset environment which is not done. While the monitor is active for CartPole-v0, you cannot call reset() unless the episode is over.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7a9ce38f362a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Train the agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_thetas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Iteration: %2i; Iteration mean reward: %7.3f'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_mean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-04e522745ec2>\u001b[0m in \u001b[0;36mcem\u001b[0;34m(f, num_steps, th_mean, batch_size, n_iter, elite_frac, initial_std)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Evaluate each sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mth\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# Select top best samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-04e522745ec2>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Evaluate each sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mth\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# Select top best samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-04e522745ec2>\u001b[0m in \u001b[0;36magent_eval\u001b[0;34m(theta, num_steps)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBinaryActionLinearPolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mrew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_rollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-04e522745ec2>\u001b[0m in \u001b[0;36mdo_rollout\u001b[0;34m(agent, env, num_steps, render)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mtotal_rew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/[/home/david/Programs/anaconda3]/envs/mtrl/lib/python3.6/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/[/home/david/Programs/anaconda3]/envs/mtrl/lib/python3.6/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36m_before_reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_before_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_after_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/[/home/david/Programs/anaconda3]/envs/mtrl/lib/python3.6/site-packages/gym/wrappers/monitoring/stats_recorder.py\u001b[0m in \u001b[0;36mbefore_reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tried to reset environment which is not done. While the monitor is active for {}, you cannot call reset() unless the episode is over.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mError\u001b[0m: Tried to reset environment which is not done. While the monitor is active for CartPole-v0, you cannot call reset() unless the episode is over."
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "from os import path\n",
    "\n",
    "# Environment Config.\n",
    "display = False\n",
    "env_id = \"CartPole-v0\"\n",
    "env = gym.make(env_id) # create environment\n",
    "env.seed(0)\n",
    "outdir = './results/cem-binary-cartPole'\n",
    "# env = wrappers.Monitor(env, outdir, force=True) #Used to save log data and video\n",
    "\n",
    "# Learning Algorithm Config.\n",
    "np.random.seed(0)\n",
    "params = dict(n_iter=20, batch_size=30, elite_frac = 0.2)\n",
    "num_thetas = env.observation_space.shape[0]+1 # plus 1 to create bias term\n",
    "num_steps = 100 # number of time steps per episode\n",
    "\n",
    "# Train the agent\n",
    "\n",
    "for (i, iterdata) in enumerate(cem(agent_eval, num_steps, np.zeros(num_thetas), **params)):\n",
    "    print('Iteration: %2i; Iteration mean reward: %7.3f'%(i, iterdata['y_mean']))\n",
    "    \n",
    "    if display and i%3==0: \n",
    "        agent = BinaryActionLinearPolicy(iterdata['theta_mean'])\n",
    "        do_rollout(agent, env, num_steps, render=True)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## CEM (Continuous Policy - Pendulum & MountainCar) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-07T13:26:48.535287Z",
     "start_time": "2018-06-07T13:26:48.132093Z"
    },
    "code_folding": [
     2,
     13
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ContinuousActionLinearPolicy(object):\n",
    "    def __init__(self, theta, n_in, n_out):\n",
    "        assert len(theta) == (n_in + 1) * n_out\n",
    "        self.W = theta[0 : n_in * n_out].reshape(n_in, n_out)\n",
    "        self.b = theta[n_in * n_out : None].reshape(1, n_out)\n",
    "        \n",
    "    def act(self, ob):\n",
    "        a = ob.dot(self.W) + self.b\n",
    "        return a\n",
    "\n",
    "    \n",
    "def cem(f,n_in, n_out, num_steps, th_mean, batch_size, n_iter, elite_frac, initial_std=1.0):\n",
    "    \"\"\"\n",
    "    Generic implementation of the cross-entropy method for maximizing a black-box function\n",
    "    f: a function mapping from vector -> scalar (i.e. from thetas --> total Reward)\n",
    "    num_steps: number of time steps that 'f' will use to evaluate each parameter\n",
    "    th_mean: initial mean of parameters (thetas)\n",
    "    batch_size: number of samples of theta to evaluate per iteration\n",
    "    n_iter: number of iterations\n",
    "    elite_frac: each iteration, select this fraction of the top-performing samples\n",
    "    initial_std: initial standard deviation over parameter (theta) vectors\n",
    "    \"\"\"\n",
    "    \n",
    "    n_elite = int(np.round(batch_size*elite_frac))\n",
    "    th_std = np.ones_like(th_mean) * initial_std # each individual theta std.\n",
    "\n",
    "    for _ in range(n_iter):\n",
    "        \n",
    "        # ths: batch_size x th_mean.size matrix containing in each row a set of\n",
    "        # parameters to be evaluated \n",
    "        ths = np.array([th_mean + dth for dth in  th_std[None,:]*np.random.randn(batch_size, th_mean.size)])\n",
    "        \n",
    "        # Evaluate each sample\n",
    "        ys = np.array([f(th, n_in, n_out, num_steps) for th in ths])\n",
    "        \n",
    "        # Select top best samples\n",
    "        elite_inds = ys.argsort()[::-1][:n_elite]\n",
    "        elite_ths = ths[elite_inds]\n",
    "\n",
    "        # Compute new mean and std. by obtaining the avg. of the top (elite) samples\n",
    "        th_mean = elite_ths.mean(axis=0)\n",
    "        th_std = elite_ths.std(axis=0)\n",
    "        yield {'ys' : ys, 'theta_mean' : th_mean, 'y_mean' : ys.mean()}\n",
    "        \n",
    "        \n",
    "def do_rollout(agent, env, num_steps, render=False):\n",
    "    \"\"\"\n",
    "    Simulate current agent/policy for 'num_steps' and return the total (undisccounted)\n",
    "    reward and elapsed time steps before episode ended\n",
    "    \"\"\"\n",
    "    \n",
    "    total_rew = 0\n",
    "    ob = env.reset()\n",
    "    for t in range(num_steps):\n",
    "        a = agent.act(ob)[0]\n",
    "        ob, reward, done, _info = env.step(a)\n",
    "        total_rew += reward\n",
    "        if render and t%2==0: env.render(mode='human')#rgb_array\n",
    "        if done: break\n",
    "            \n",
    "    return total_rew, t+1       \n",
    "\n",
    "\n",
    "def agent_eval_cont(theta, n_in, n_out, num_steps):\n",
    "    \"\"\"\n",
    "    Given a policy (theta), evaluate it by simulating a single episode runned by\n",
    "    'num_steps' time steps of the simulation,and obtaining the total reward\n",
    "    \"\"\"\n",
    "    agent = ContinuousActionLinearPolicy(theta, n_in, n_out)\n",
    "    rew, T = do_rollout(agent, env, num_steps)\n",
    "    \n",
    "    return rew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-07T13:15:27.204992Z",
     "start_time": "2018-06-07T13:14:49.655831Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Iteration:  0; Iteration mean reward: -1484.572\n",
      "Iteration:  1; Iteration mean reward: -1586.916\n",
      "Iteration:  2; Iteration mean reward: -1485.845\n",
      "Iteration:  3; Iteration mean reward: -1400.281\n",
      "Iteration:  4; Iteration mean reward: -1370.339\n",
      "Iteration:  5; Iteration mean reward: -1279.676\n",
      "Iteration:  6; Iteration mean reward: -1266.759\n",
      "Iteration:  7; Iteration mean reward: -1186.064\n",
      "Iteration:  8; Iteration mean reward: -1269.237\n",
      "Iteration:  9; Iteration mean reward: -1265.276\n",
      "Iteration: 10; Iteration mean reward: -1173.775\n",
      "Iteration: 11; Iteration mean reward: -1223.677\n",
      "Iteration: 12; Iteration mean reward: -1205.208\n",
      "Iteration: 13; Iteration mean reward: -1215.291\n",
      "Iteration: 14; Iteration mean reward: -1177.453\n",
      "Iteration: 15; Iteration mean reward: -1227.330\n",
      "Iteration: 16; Iteration mean reward: -1126.090\n",
      "Iteration: 17; Iteration mean reward: -1132.339\n",
      "Iteration: 18; Iteration mean reward: -1078.917\n",
      "Iteration: 19; Iteration mean reward: -1137.810\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-4728c0049ab3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Train the agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_eval_cont\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_thetas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Iteration: %2i; Iteration mean reward: %7.3f'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_mean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-2b9cce491bf7>\u001b[0m in \u001b[0;36mcem\u001b[0;34m(f, n_in, n_out, num_steps, th_mean, batch_size, n_iter, elite_frac, initial_std)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Evaluate each sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mth\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Select top best samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-2b9cce491bf7>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Evaluate each sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mth\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Select top best samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-2b9cce491bf7>\u001b[0m in \u001b[0;36magent_eval_cont\u001b[0;34m(theta, n_in, n_out, num_steps)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \"\"\"\n\u001b[1;32m     71\u001b[0m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mContinuousActionLinearPolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mrew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_rollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-2b9cce491bf7>\u001b[0m in \u001b[0;36mdo_rollout\u001b[0;34m(agent, env, num_steps, render)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mtotal_rew\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrender\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#rgb_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/[/home/david/Programs/anaconda3]/envs/mlp/lib/python3.6/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/[/home/david/Programs/anaconda3]/envs/mlp/lib/python3.6/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36m_after_step\u001b[0;34m(self, observation, reward, done, info)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;31m# Record video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/[/home/david/Programs/anaconda3]/envs/mlp/lib/python3.6/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36mcapture_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mrender_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ansi'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mansi_mode\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrender_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/[/home/david/Programs/anaconda3]/envs/mlp/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/[/home/david/Programs/anaconda3]/envs/mlp/lib/python3.6/site-packages/gym/envs/classic_control/pendulum.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgtrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_u\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_u\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/[/home/david/Programs/anaconda3]/envs/mlp/lib/python3.6/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_rgb_array\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mglClearColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/[/home/david/Programs/anaconda3]/envs/mlp/lib/python3.6/site-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36mclear\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1226\u001b[0m         \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mThe\u001b[0m \u001b[0mwindow\u001b[0m \u001b[0mmust\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mactive\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msee\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \"\"\"\n\u001b[0;32m-> 1228\u001b[0;31m         \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglClear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGL_COLOR_BUFFER_BIT\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGL_DEPTH_BUFFER_BIT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdispatch_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/[/home/david/Programs/anaconda3]/envs/mlp/lib/python3.6/site-packages/pyglet/gl/lib.py\u001b[0m in \u001b[0;36merrcheck\u001b[0;34m(result, func, arguments)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0merrcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_debug_gl_trace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "from os import path\n",
    "\n",
    "# Environment Config.\n",
    "display = False\n",
    "env_id = \"Pendulum-v0\"\n",
    "env = gym.make(env_id) # create environment\n",
    "env.seed(0)\n",
    "outdir = './results/cem-cont-pendulum'\n",
    "env = wrappers.Monitor(env, outdir, force=True) #Used to save log data and video\n",
    "\n",
    "# Learning Algorithm Config.\n",
    "np.random.seed(0)\n",
    "params = dict(n_iter=500, batch_size=25, elite_frac = 0.3)\n",
    "num_steps = 300 # number of time steps per episode\n",
    "n_in = env.observation_space.shape[0]\n",
    "n_out = env.action_space.shape[0]\n",
    "num_thetas = (n_in + 1) * n_out # plus 1 to create bias term\n",
    "\n",
    "# Train the agent\n",
    "for (i, iterdata) in enumerate(cem(agent_eval_cont, n_in, n_out, num_steps, np.zeros(num_thetas), **params)):\n",
    "    print('Iteration: %2i; Iteration mean reward: %7.3f'%(i, iterdata['y_mean']))\n",
    "    \n",
    "    if display and i%10==0: \n",
    "        agent = ContinuousActionLinearPolicy(iterdata['theta_mean'], n_in, n_out)\n",
    "        do_rollout(agent, env, num_steps, render=True)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-10T10:34:23.794755Z",
     "start_time": "2018-06-10T10:34:23.730876Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wrappers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-40c8e98c600e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_simu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Used to save log data and video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_simu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mContinuousActionLinearPolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'theta_mean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wrappers' is not defined"
     ]
    }
   ],
   "source": [
    "num_simu = 5\n",
    "env = wrappers.Monitor(env, outdir, force=True) #Used to save log data and video\n",
    "\n",
    "for i in range(num_simu):\n",
    "    agent = ContinuousActionLinearPolicy(iterdata['theta_mean'], n_in, n_out)\n",
    "    do_rollout(agent, env, num_steps, render=True)\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-07T13:26:29.932533Z",
     "start_time": "2018-06-07T13:25:29.880822Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Iteration:  0; Iteration mean reward: -26.275\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-33d82916e542>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Train the agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_eval_cont\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_thetas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Iteration: %2i; Iteration mean reward: %7.3f'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_mean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-dc5cf6c25e95>\u001b[0m in \u001b[0;36mcem\u001b[0;34m(f, n_in, n_out, num_steps, th_mean, batch_size, n_iter, elite_frac, initial_std)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Evaluate each sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mth\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Select top best samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-dc5cf6c25e95>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Evaluate each sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mth\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Select top best samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-dc5cf6c25e95>\u001b[0m in \u001b[0;36magent_eval_cont\u001b[0;34m(theta, n_in, n_out, num_steps)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mContinuousActionLinearPolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mrew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_rollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-dc5cf6c25e95>\u001b[0m in \u001b[0;36mdo_rollout\u001b[0;34m(agent, env, num_steps, render)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mtotal_rew\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrender\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#rgb_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/[/home/david/Programs/anaconda3]/envs/mlp/lib/python3.6/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/[/home/david/Programs/anaconda3]/envs/mlp/lib/python3.6/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36m_after_step\u001b[0;34m(self, observation, reward, done, info)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;31m# Record video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/[/home/david/Programs/anaconda3]/envs/mlp/lib/python3.6/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36mcapture_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_ansi_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_image_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/[/home/david/Programs/anaconda3]/envs/mlp/lib/python3.6/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36m_encode_image_frame\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidFrame\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tried to pass invalid video frame, marking as broken: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/[/home/david/Programs/anaconda3]/envs/mlp/lib/python3.6/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36mcapture_frame\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdistutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLooseVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mdistutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLooseVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1.9.0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "from os import path\n",
    "\n",
    "# Environment Config.\n",
    "display = False\n",
    "env_id = \"MountainCarContinuous-v0\"\n",
    "env = gym.make(env_id) # create environment\n",
    "env.seed(0)\n",
    "outdir = './results/cem-cont-mountainCar'\n",
    "env = wrappers.Monitor(env, outdir, force=True) #Used to save log data and video\n",
    "\n",
    "# Learning Algorithm Config.\n",
    "np.random.seed(0)\n",
    "params = dict(n_iter=500, batch_size=100, elite_frac = 0.5)\n",
    "num_steps = 300 # number of time steps per episode\n",
    "n_in = env.observation_space.shape[0]\n",
    "n_out = env.action_space.shape[0]\n",
    "num_thetas = (n_in + 1) * n_out # plus 1 to create bias term\n",
    "\n",
    "# Train the agent\n",
    "for (i, iterdata) in enumerate(cem(agent_eval_cont, n_in, n_out, num_steps, np.zeros(num_thetas), **params)):\n",
    "    print('Iteration: %2i; Iteration mean reward: %7.3f'%(i, iterdata['y_mean']))\n",
    "    \n",
    "    if display and i%10==0: \n",
    "        agent = ContinuousActionLinearPolicy(iterdata['theta_mean'], n_in, n_out)\n",
    "        do_rollout(agent, env, num_steps, render=True)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Policy Gradient w/ Pong (Based on Andrej Kaparthy's Code) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T08:03:20.878660Z",
     "start_time": "2018-06-12T08:03:19.153467Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Trains an agent with (stochastic) Policy Gradients on Pong. Uses OpenAI Gym. \n",
    " - Orignal Code by Andrek Kaparthy: https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5\n",
    " - Blog explainning Code: http://karpathy.github.io/2016/05/31/rl/\n",
    " - Code in TF: https://github.com/gameofdimension/policy-gradient-pong/blob/master/policy_gradient_pong.py\n",
    " - Code multiple actions w/ Softmax: https://gist.github.com/etienne87/6803a65653975114e6c6f08bb25e1522\n",
    " - Actor Mimic version: https://github.com/schinger/pong_actor-critic\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import os\n",
    "\n",
    "# General settings\n",
    "log_level = 0 # 0:don't print anything ; 1:print fundamental things; 2: print everything\n",
    "\n",
    "load_agent_dir = \"./results/pong_agents\"\n",
    "# agent_to_load = \"pongAgent_ep_100_epTotR_-20.0_runningAvgR_-20.999.p\" # Basic (shit) Agent\n",
    "# agent_to_load = \"pongAgent_ep_1900_meanR_-14_runningAvgR_-11.5.p\" # Intermediate Agent\n",
    "agent_to_load = \"pongAgent_ep_5100_epTotR_-6.0_runningAvgR_-0.3960004310609734.p\" # Further trainned agent\n",
    "\n",
    "resume = True # resume from previous checkpoint\n",
    "simulate = False # Test trainned agent in simulation \n",
    "render = False # Show video window of environment simulation\n",
    "save_video = False # save video when simulating agent\n",
    "num_episodes = 1 # number of episodes to simulate agent for\n",
    "\n",
    "\n",
    "# Policy Network Hyperparameters\n",
    "H = 200 # number of hidden layer neurons\n",
    "batch_size = 10 # every how many episodes to do a param update\n",
    "# learning_rate = 1e-3 # For initial 4000 episodes\n",
    "learning_rate = 3e-4 # For further episodes\n",
    "gamma = 0.99 # discount factor for reward\n",
    "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
    "\n",
    "\n",
    "# Model Initialization\n",
    "input_D = 80 * 80 # input dimensionality: 80x80 grid\n",
    "if resume: # Load trainned agent\n",
    "    model = pickle.load(open(load_agent_dir + \"/\" + agent_to_load, 'rb'))\n",
    "else:\n",
    "    model = {} # Dictionary storing Policy Neural Network parameters\n",
    "    model['W1'] = np.random.randn(H,input_D) / np.sqrt(input_D) # \"Xavier\" initialization\n",
    "    model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
    "    \n",
    "# model.items() returns a touple of Keys ('W1') and values (initial weight values)\n",
    "grad_buffer = { key : np.zeros_like(val) for key,val in model.items() } # update buffers that add up gradients over a batch\n",
    "rmsprop_cache = { key : np.zeros_like(val) for key,val in model.items() } # rmsprop memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T08:03:24.937770Z",
     "start_time": "2018-06-12T08:03:24.365656Z"
    },
    "code_folding": [
     0,
     4,
     14,
     25,
     36,
     55,
     78
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x): \n",
    "    return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to probability --> interval [0,1]\n",
    "\n",
    "\n",
    "def prepro(I):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "    I = I[35:195] # crop\n",
    "    I = I[::2,::2,0] # downsample by factor of 2 and only use Red channel (could had choosen any channel since in the following lines we binarize everything)\n",
    "    I[I == 144] = 0 # erase background (background type 1)\n",
    "    I[I == 109] = 0 # erase background (background type 2)\n",
    "    I[I != 0] = 1 # everything else (paddles, ball) just set to 1 (binarize)\n",
    "    return I.astype(np.float).ravel() # ravel() == flatten\n",
    "\n",
    "\n",
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "\n",
    "\n",
    "def policy_forward(x):\n",
    "    # Neural Network performing logistic regresion, hence, having a sigmoid activation function at the\n",
    "    # output layer establishing P(y=1 | x;w,b)\n",
    "    act_hid = np.dot(model['W1'], x)\n",
    "    h = act_hid*(act_hid>0) + 0*(act_hid<=0) # ReLU nonlinearity\n",
    "    logp = np.dot(model['W2'], h)\n",
    "    p = sigmoid(logp) # P(y=1 | x;w,b)\n",
    "    \n",
    "    return p, h # return probability of taking action 2, and hidden state\n",
    "   \n",
    "    \n",
    "def policy_backward(eph, epdlogp, epx):  # ONLY THING PEDING TO FULLY UNDERSTAND --> DIMENSIONS OF DW2 AND DW1 ?\n",
    "    \"\"\" \n",
    "    backward pass: \n",
    "    eph --> Episodes Hidden --> array of intermediate hidden states\n",
    "    epdlogp --> Episodes gradient --> array containing the gradients dloss/dlogp = dL/dactivation_of_sigmoid)\n",
    "    epx --> Episodes input features\n",
    "    \"\"\"\n",
    "    # Computing each gradient through Reverse Mode Differiantation (a.k.a. Backpropagation)\n",
    "    # Note that each WEight gradient is computed based on the sum of epdlogp gradients across all time steps in episode\n",
    "    dW2 = np.dot(eph.T, epdlogp).ravel() # dL/dW2 = dL/dlogp * dlogp/dW2 = (y-p)*h = epdlogp*eph\n",
    "    dh = np.outer(epdlogp, model['W2']) # dL/dh = dL/dlogp * dlogp/dh = (y-p)*W2 = epdlogp*W2\n",
    "    dact_hid = dh*(eph > 0) + 0*(eph <= 0) # backprop. ReLu\n",
    "    dW1 = np.dot(dact_hid.T, epx) # dL/dW1 = dL/dact_hid * dact_hid/dW1 = (...)*x = dact_hid*x\n",
    "    \n",
    "    return {'W1':dW1, 'W2':dW2}\n",
    "\n",
    "\n",
    "\n",
    "# Methods to test/simulate agent\n",
    "def act (observation, prev_x):\n",
    "    \"\"\"\n",
    "    Selects action taken by trainned agent based on (preproccessed difference) of observations\n",
    "    Input:\n",
    "        * observation (np array): Matrix containing the raw RGB image of the current state of the game\n",
    "        * prev_x (np array): previous preproccessed game frame\n",
    "    \n",
    "    Output:\n",
    "        * action (int): action selected by policy network\n",
    "        * prev_x (np array): updated previous preproccessed game frame\n",
    "    \"\"\"\n",
    "    # Get preprocesses image + compute difference between frames\n",
    "    cur_x = prepro(observation)\n",
    "    x = cur_x - prev_x if prev_x is not None else np.zeros(input_D)\n",
    "    prev_x = cur_x # Save current image for next environment step\n",
    "    \n",
    "    # Select action\n",
    "    a_prob, _ = policy_forward(x)\n",
    "    action = 2 if np.random.uniform() < a_prob else 3 # roll the dice! to select action\n",
    "    \n",
    "    return action, prev_x\n",
    "    \n",
    "    \n",
    "def sim_agent(env, num_episodes, render=False, save_video=False):\n",
    "    \"\"\"\n",
    "    Simulates trainned agent (i.e. model with NN's weights) in given environment (env)\n",
    "    Input:\n",
    "        * env (Open AI gym object): Open Ai Gym Environment\n",
    "        * num_episodes (int): number of episodes (in Pong, one episoe= 21 games) to simulate\n",
    "        * render (bool): determines if video should be rendered in window\n",
    "        * save_video (bool): enables saving video and other stats of simulated episodes\n",
    "        \n",
    "    Returns:\n",
    "        * mean_reward_episodes (double): Mean reward obtained across all episodes\n",
    "        * mean_won_episodes (double): Mean number of episodes won \n",
    "        * if save_video=True, stores videos and stats in folder determined by 'outdir'\n",
    "    \"\"\" \n",
    "    \n",
    "    # General Config\n",
    "    episodes_tot_reward = []\n",
    "    episodes_perc_won = []\n",
    "    \n",
    "    # Monitoring Config\n",
    "    if save_video: \n",
    "        outdir = './results/pg-binary-Pong'\n",
    "        if not os.path.exists(outdir):  os.makedirs(outdir) # create directory if it doesn't exist\n",
    "        env = wrappers.Monitor(env, outdir, force=True) # Used to save log data and video\n",
    "        \n",
    "    # Simulate each Episode\n",
    "    for episode in range(num_episodes):\n",
    "        print(\"\\nEpisode No.: {}\".format(episode))\n",
    "        observation = env.reset()\n",
    "        prev_x = None # variable storing previous (preprocessed) observation\n",
    "        reward_sum = 0\n",
    "        reward_list = []\n",
    "        \n",
    "        while True:\n",
    "            if render: env.render()\n",
    "            action, prev_x = act(observation, prev_x) # Select action based on  PMF parametrized by agent/NN weights\n",
    "            \n",
    "            observation, reward, done, _ = env.step(action)\n",
    "            reward_sum += reward\n",
    "            if reward !=0: reward_list.append(reward)\n",
    "            \n",
    "            if done:\n",
    "                perc_won = 100*np.sum(np.array(reward_list)>0)/len(reward_list)\n",
    "                episodes_tot_reward.append(reward_sum)\n",
    "                episodes_perc_won.append(perc_won)\n",
    "                \n",
    "                print(\"Episode {} finished ; Total Reward: {} ; Perc. Matches Won: {}\\n\\n\"\\\n",
    "                      .format(episode, reward_sum,  perc_won ))\n",
    "\n",
    "                break\n",
    "\n",
    "    env.close() # Close windows (and also destroys environment :( )\n",
    "    mean_reward_episodes = np.mean(episodes_tot_reward)\n",
    "    mean_won_episodes = np.mean(episodes_perc_won)\n",
    "    \n",
    "    return mean_reward_episodes, mean_won_episodes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-12T08:08:42.982691Z",
     "start_time": "2018-06-12T08:07:55.700773Z"
    },
    "code_folding": [
     10,
     24
    ],
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"Pong-v0\")\n",
    "\n",
    "# Variable already stablished ontop --> just having them here for quick changes\n",
    "log_level = 1\n",
    "# render = False\n",
    "# simulate = False # Test trainned agent in simulation \n",
    "# save_video = True # save video when simulating agent\n",
    "# num_episodes = 1 # number of episodes to simulate agent for\n",
    "\n",
    "# SIMULATE TRAINED AGENT\n",
    "if simulate: # simulate pre-trainned loaded agent\n",
    "    mean_reward_episodes, mean_won_episodes = sim_agent(env, num_episodes, render, save_video)\n",
    "    print(\"Mean Reward: {} ; Mean Perc. Won: {}\".format(mean_reward_episodes, mean_won_episodes))\n",
    "\n",
    "# TRAIN AGENT \n",
    "else: # Train agent (Policy Network)       \n",
    "    \n",
    "    observation = env.reset()\n",
    "    prev_x = None # variable storing previous observation (image)\n",
    "    xs,hs,dlogps,drs = [],[],[],[] # Stores episode trajectory: observations, NN Hidden state, gradients, rewards \n",
    "    running_reward = None\n",
    "    reward_sum = 0\n",
    "    episode_number = 0\n",
    "\n",
    "    while True:\n",
    "        if render: env.render()\n",
    "\n",
    "        # PREPROCESS: Set input to network to be difference between images\n",
    "        cur_x = prepro(observation)\n",
    "        x = cur_x - prev_x if prev_x is not None else np.zeros(input_D)\n",
    "        prev_x = cur_x # Save current image for next environment step\n",
    "\n",
    "        # FORWARD POLICY NN: forward the policy network and sample an action from the returned probability\n",
    "        a_prob, h = policy_forward(x) # Get probability of action 2 (this assumption is established by the programmer, and could be\n",
    "                                      # switched to action 3) given current state/observation/image\n",
    "        action = 2 if np.random.uniform() < a_prob else 3 # roll the dice! to select action\n",
    "\n",
    "        # CACHING FOR BACKPROP: Store time step variables needed for backprop ... here we are basically\n",
    "        # appending data points and whe done==True, we will update the params for the COMPLETE BATCH\n",
    "        xs.append(x) # observation stored\n",
    "        hs.append(h) # hidden state stored\n",
    "\n",
    "        # Create \"fake\" label pushing the NN to make the action it just choose more probable\n",
    "        # see EC1 at the End of Code for detailed explenation\n",
    "        y = 1 if action == 2 else 0 \n",
    "\n",
    "        # Gradient of Negative Log Likelihood Loss function w.r.t activation of sigmoid (i.e. logp)\n",
    "        # see EC2 at the End of Code for detailed explenation\n",
    "        dlogps.append(y - a_prob) \n",
    "\n",
    "\n",
    "        # STEP ENV: get new measurements\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        reward_sum += reward\n",
    "        drs.append(reward) # record reward \n",
    "\n",
    "        if done: # an episode finished\n",
    "            episode_number += 1\n",
    "\n",
    "            # STACK EPISODE VARIABLES: Stack inputs, hidden states,  gradients, and rewards \n",
    "            epx = np.vstack(xs) # Episode Inputs\n",
    "            eph = np.vstack(hs)\n",
    "            epdlogp = np.vstack(dlogps)\n",
    "            epr = np.vstack(drs) # Episode Rewards\n",
    "            xs,hs,dlogps,drs = [],[],[],[] # RESET array memory\n",
    "\n",
    "            # COMPUTE EPISODE'S DISCCOUNTED REWARD: returning a list, done for each time step in episode\n",
    "            discounted_epr = discount_rewards(epr)\n",
    "            discounted_epr -= np.mean(discounted_epr) # Standardize (mu=0, std=1)\n",
    "            discounted_epr /= np.std(discounted_epr)\n",
    "\n",
    "            # SCORE FUNCCTION GRADIENT ESTIMATOR: the group of the episodes gradients stored in 'epdlogp' pushes the\n",
    "            # parameters towards increasing the prob. of selecting the actions it selected\n",
    "            # during the Episode. However, we modulate this gradients movements by how good it was to take\n",
    "            # each action; based on the advantage (disccounted reward) at each time step\n",
    "            epdlogp *= discounted_epr # Policy Gradient (PG) magic happens right here !\n",
    "            grad = policy_backward(eph, epdlogp, epx)# Compute gradient w.r.t  Weights for current Episode\n",
    "            for key in model: grad_buffer[key] += grad[key] # Accumulate grad. over all batches\n",
    "\n",
    "            # RMSPROP: perform update every \"batch_size\" episodes\n",
    "            if episode_number % batch_size == 0:\n",
    "                for key,val in model.items():\n",
    "                    g = grad_buffer[key] # gradient accumulated thorugh all batches\n",
    "                    rmsprop_cache[key] = decay_rate * rmsprop_cache[key] + (1 - decay_rate) * g**2\n",
    "                    model[key] += learning_rate * g / (np.sqrt(rmsprop_cache[key]) + 1e-5) # UPDATE model params.  \n",
    "                    grad_buffer[key] = np.zeros_like(val) # reset batch gradient buffer\n",
    "\n",
    "            # Information Variables and Storing Model\n",
    "            running_reward =  reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "            if log_level>=1: print ('Resetting Env --> Episode: %i ; Tot. Reward: %f ; Current Running mean: %f' % (episode_number, reward_sum, running_reward))\n",
    "            if episode_number % 100 == 0: \n",
    "                pickle.dump(model, open(load_agent_dir + \"/\" + 'pongAgent_ep_{}_epTotR_{}_runningAvgR_{}.p'.format(episode_number, reward_sum, running_reward), 'wb'))\n",
    "                if log_level>=1: print (\"Saved Model at Episode %i \\n\\n\" % (episode_number))\n",
    "            \n",
    "            # RESET VARIABLES\n",
    "            reward_sum = 0\n",
    "            observation = env.reset() # reset env\n",
    "            prev_x = None\n",
    "\n",
    "        if reward != 0 and log_level>=2: # Print info when the game ends.\n",
    "            print (\"Episode: {} - Outcome: {} \".format(episode_number, \"LOST\" if reward == -1 else \"WON\"))\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "********* Extended Comments (EC) **********\n",
    "\n",
    "* EC1: Fake label for Score Function Gradient Estimator\n",
    "Create \"fake\" target label, in which with out mattering if the chossen 'action' gave us a high/low\n",
    "reward, we want the NN to make the that action more likely (i.e. action=2, make it more likely by\n",
    "p--> 1; if action=3, make it more likely by pushing p-->0) ...its naive to believe that making any\n",
    "action choosen by the non-optimal NN more probable will in turn improve the agent, HOWEVER, we will\n",
    "multiply the gradient by the score function (advantage function, which is the disccounted reward \n",
    "for each time step), and multiplying the gradient by this will indeed move our NN's parameters\n",
    "in the direction maximizing the Expected value of the score/advange function, which is what we want!\n",
    "(see details in \"score function gradient estimator\" section of karpathy.github.io/2016/05/31/rl/)\n",
    "\n",
    "* EC2: Gradient for Maximizing (log) Likelihood of the model / Minimizing Negative Log Likelihood:\n",
    "Compute gradient --> this is the gradient of the Loss function w.r.t activation of sigmoid(logp = W2.h).\n",
    "The loss function we are minimizing is L = -log(likelihood) i.e. Negative Log Likelihood, which \n",
    "in turn is equivalent to maximizing the (log) likelihood of the data (see more on L and derivation of \n",
    "dL/dp at MLPR Uni. of Edinburgh notes or at  http://cs231n.github.io/neural-networks-2/#losses)\n",
    "\n",
    "********* Extended Comments (EC) **********\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Proximal Policy Optimization (PPO, based on Patrick Coady's Code) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:44:46.733454Z",
     "start_time": "2018-06-13T22:44:26.529382Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-91e34c825480>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/[/home/david/Programs/anaconda3]/envs/mtrl/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/[/home/david/Programs/anaconda3]/envs/mtrl/lib/python3.6/site-packages/gym/envs/box2d/bipedal_walker.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_polyline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/[/home/david/Programs/anaconda3]/envs/mtrl/lib/python3.6/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mgeom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgeom\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monetime_geoms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mgeom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/[/home/david/Programs/anaconda3]/envs/mtrl/lib/python3.6/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0mattr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mattr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/[/home/david/Programs/anaconda3]/envs/mtrl/lib/python3.6/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mrender1\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mglBegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGL_LINE_LOOP\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mGL_LINE_STRIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m             \u001b[0mglVertex3f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# draw each vertex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0mglEnd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_linewidth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env.env.world.gravity = (0,-200)\n",
    "# env = gym.make(\"CarRacing-v0\")\n",
    "\n",
    "env.reset()\n",
    "for t in range(1000):\n",
    "    env.render()\n",
    "    a = env.action_space.sample()\n",
    "    obs, rew, done, info = env.step(a)\n",
    "    if done: break\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
